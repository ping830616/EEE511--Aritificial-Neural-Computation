{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m34\u001b[0m\n\u001b[1;33m    def recognize(self, inputs: Tensor, input_lengths: Tensor):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from kospeech.models.modules import Linear\n",
    "from kospeech.models.encoder import (\n",
    "    BaseEncoder,\n",
    "    TransducerEncoder,\n",
    ")\n",
    "from kospeech.models.decoder import (\n",
    "    BaseDecoder,\n",
    "    TransducerDecoder,\n",
    ")\n",
    "\n",
    "\n",
    "[docs]class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "[docs]    def count_parameters(self) -> int:\n",
    "        \"\"\" Count parameters of encoder \"\"\"\n",
    "        return sum([p.numel for p in self.parameters()])\n",
    "\n",
    "\n",
    "[docs]    def update_dropout(self, dropout_p: float) -> None:\n",
    "        \"\"\" Update dropout probability of encoder \"\"\"\n",
    "        for name, child in self.named_children():\n",
    "            if isinstance(child, nn.Dropout):\n",
    "                child.p = dropout_p\n",
    "\n",
    "\n",
    "    #torch.no_grad()\n",
    "    def recognize(self, inputs: Tensor, input_lengths: Tensor):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "[docs]class EncoderModel(BaseModel):\n",
    "    \"\"\" Super class of KoSpeech's Encoder only Models \"\"\"\n",
    "    def __init__(self):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        self.decoder = None\n",
    "\n",
    "[docs]    def set_decoder(self, decoder):\n",
    "        \"\"\" Setter for decoder \"\"\"\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "[docs]    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Forward propagate a `inputs` for  ctc training.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "\n",
    "        Returns:\n",
    "            (Tensor, Tensor):\n",
    "\n",
    "            * predicted_log_prob (torch.FloatTensor)s: Log probability of model predictions.\n",
    "            * output_lengths (torch.LongTensor): The length of output tensor ``(batch)``\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "[docs]    @torch.no_grad()\n",
    "    def decode(self, predicted_log_probs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Decode encoder_outputs.\n",
    "\n",
    "        Args:\n",
    "            predicted_log_probs (torch.FloatTensor):Log probability of model predictions. `FloatTensor` of size\n",
    "                ``(batch, seq_length, dimension)``\n",
    "\n",
    "        Returns:\n",
    "            * predictions (torch.FloatTensor): Result of model predictions.\n",
    "        \"\"\"\n",
    "        return predicted_log_probs.max(-1)[1]\n",
    "\n",
    "\n",
    "[docs]    @torch.no_grad()\n",
    "    def recognize(self, inputs: Tensor, input_lengths: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Recognize input speech.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "\n",
    "        Returns:\n",
    "            * predictions (torch.FloatTensor): Result of model predictions.\n",
    "        \"\"\"\n",
    "        predicted_log_probs, _ = self.forward(inputs, input_lengths)\n",
    "        if self.decoder is not None:\n",
    "            return self.decoder.decode(predicted_log_probs)\n",
    "        return self.decode(predicted_log_probs)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class EncoderDecoderModel(BaseModel):\n",
    "    \"\"\" Super class of KoSpeech's Encoder-Decoder Models \"\"\"\n",
    "    def __init__(self, encoder: BaseEncoder, decoder: BaseDecoder) -> None:\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "[docs]    def set_encoder(self, encoder):\n",
    "        \"\"\" Setter for encoder \"\"\"\n",
    "        self.encoder = encoder\n",
    "\n",
    "\n",
    "[docs]    def set_decoder(self, decoder):\n",
    "        \"\"\" Setter for decoder \"\"\"\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "[docs]    def count_parameters(self) -> int:\n",
    "        \"\"\" Count parameters of encoder \"\"\"\n",
    "        num_encoder_parameters = self.encoder.count_parameters()\n",
    "        num_decoder_parameters = self.decoder.count_parameters()\n",
    "        return num_encoder_parameters + num_decoder_parameters\n",
    "\n",
    "\n",
    "[docs]    def update_dropout(self, dropout_p) -> None:\n",
    "        \"\"\" Update dropout probability of model \"\"\"\n",
    "        self.encoder.update_dropout(dropout_p)\n",
    "        self.decoder.update_dropout(dropout_p)\n",
    "\n",
    "\n",
    "[docs]    def forward(\n",
    "            self,\n",
    "            inputs: Tensor,\n",
    "            input_lengths: Tensor,\n",
    "            targets: Tensor,\n",
    "            *args,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Forward propagate a `inputs` and `targets` pair for training.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "            targets (torch.LongTensr): A target sequence passed to decoder. `IntTensor` of size ``(batch, seq_length)``\n",
    "\n",
    "        Returns:\n",
    "            (Tensor, Tensor, Tensor)\n",
    "\n",
    "            * predicted_log_probs (torch.FloatTensor): Log probability of model predictions.\n",
    "            * encoder_output_lengths: The length of encoder outputs. ``(batch)``\n",
    "            * encoder_log_probs: Log probability of encoder outputs will be passed to CTC Loss.\n",
    "                If joint_ctc_attention is False, return None.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "[docs]    @torch.no_grad()\n",
    "    def recognize(self, inputs: Tensor, input_lengths: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Recognize input speech. This method consists of the forward of the encoder and the decode() of the decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "\n",
    "        Returns:\n",
    "            * predictions (torch.FloatTensor): Result of model predictions.\n",
    "        \"\"\"\n",
    "        encoder_outputs, encoder_output_lengths, _ = self.encoder(inputs, input_lengths)\n",
    "        return self.decoder.decode(encoder_outputs, encoder_output_lengths)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class TransducerModel(BaseModel):\n",
    "    \"\"\" Super class of KoSpeech's Transducer Models \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder: TransducerEncoder,\n",
    "            decoder: TransducerDecoder,\n",
    "            d_model: int,\n",
    "            num_classes: int,\n",
    "    ) -> None:\n",
    "        super(TransducerModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fc = nn.Sequential(\n",
    "            Linear(d_model << 1, d_model),\n",
    "            nn.Tanh(),\n",
    "            Linear(d_model, num_classes, bias=False),\n",
    "        )\n",
    "\n",
    "[docs]    def set_encoder(self, encoder):\n",
    "        \"\"\" Setter for encoder \"\"\"\n",
    "        self.encoder = encoder\n",
    "\n",
    "\n",
    "[docs]    def set_decoder(self, decoder):\n",
    "        \"\"\" Setter for decoder \"\"\"\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "[docs]    def count_parameters(self) -> int:\n",
    "        \"\"\" Count parameters of encoder \"\"\"\n",
    "        num_encoder_parameters = self.encoder.count_parameters()\n",
    "        num_decoder_parameters = self.decoder.count_parameters()\n",
    "        return num_encoder_parameters + num_decoder_parameters\n",
    "\n",
    "\n",
    "[docs]    def update_dropout(self, dropout_p) -> None:\n",
    "        \"\"\" Update dropout probability of model \"\"\"\n",
    "        self.encoder.update_dropout(dropout_p)\n",
    "        self.decoder.update_dropout(dropout_p)\n",
    "\n",
    "\n",
    "[docs]    def joint(self, encoder_outputs: Tensor, decoder_outputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Joint `encoder_outputs` and `decoder_outputs`.\n",
    "\n",
    "        Args:\n",
    "            encoder_outputs (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n",
    "                ``(batch, seq_length, dimension)``\n",
    "            decoder_outputs (torch.FloatTensor): A output sequence of decoder. `FloatTensor` of size\n",
    "                ``(batch, seq_length, dimension)``\n",
    "\n",
    "        Returns:\n",
    "            * outputs (torch.FloatTensor): outputs of joint `encoder_outputs` and `decoder_outputs`..\n",
    "        \"\"\"\n",
    "        if encoder_outputs.dim() == 3 and decoder_outputs.dim() == 3:\n",
    "            input_length = encoder_outputs.size(1)\n",
    "            target_length = decoder_outputs.size(1)\n",
    "\n",
    "            encoder_outputs = encoder_outputs.unsqueeze(2)\n",
    "            decoder_outputs = decoder_outputs.unsqueeze(1)\n",
    "\n",
    "            encoder_outputs = encoder_outputs.repeat([1, 1, target_length, 1])\n",
    "            decoder_outputs = decoder_outputs.repeat([1, input_length, 1, 1])\n",
    "\n",
    "        outputs = torch.cat((encoder_outputs, decoder_outputs), dim=-1)\n",
    "        outputs = self.fc(outputs).log_softmax(dim=-1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "[docs]    def forward(\n",
    "            self,\n",
    "            inputs: Tensor,\n",
    "            input_lengths: Tensor,\n",
    "            targets: Tensor,\n",
    "            target_lengths: Tensor\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward propagate a `inputs` and `targets` pair for training.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "            targets (torch.LongTensr): A target sequence passed to decoder. `IntTensor` of size ``(batch, seq_length)``\n",
    "            target_lengths (torch.LongTensor): The length of target tensor. ``(batch)``\n",
    "\n",
    "        Returns:\n",
    "            * predictions (torch.FloatTensor): Result of model predictions.\n",
    "        \"\"\"\n",
    "        encoder_outputs, _ = self.encoder(inputs, input_lengths)\n",
    "        decoder_outputs, _ = self.decoder(targets, target_lengths)\n",
    "        return self.joint(encoder_outputs, decoder_outputs)\n",
    "\n",
    "\n",
    "[docs]    @torch.no_grad()\n",
    "    def decode(self, encoder_output: Tensor, max_length: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Decode `encoder_outputs`.\n",
    "\n",
    "        Args:\n",
    "            encoder_output (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n",
    "                ``(seq_length, dimension)``\n",
    "            max_length (int): max decoding time step\n",
    "\n",
    "        Returns:\n",
    "            * predicted_log_probs (torch.FloatTensor): Log probability of model predictions.\n",
    "        \"\"\"\n",
    "        pred_tokens, hidden_state = list(), None\n",
    "        decoder_input = encoder_output.new_tensor([[self.decoder.sos_id]], dtype=torch.long)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            decoder_output, hidden_state = self.decoder(decoder_input, hidden_states=hidden_state)\n",
    "            step_output = self.joint(encoder_output[t].view(-1), decoder_output.view(-1))\n",
    "            step_output = step_output.softmax(dim=0)\n",
    "            pred_token = step_output.argmax(dim=0)\n",
    "            pred_token = int(pred_token.item())\n",
    "            pred_tokens.append(pred_token)\n",
    "            decoder_input = step_output.new_tensor([[pred_token]], dtype=torch.long)\n",
    "\n",
    "        return torch.LongTensor(pred_tokens)\n",
    "\n",
    "\n",
    "[docs]    @torch.no_grad()\n",
    "    def recognize(self, inputs: Tensor, input_lengths: Tensor):\n",
    "        \"\"\"\n",
    "        Recognize input speech. This method consists of the forward of the encoder and the decode() of the decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "\n",
    "        Returns:\n",
    "            * predictions (torch.FloatTensor): Result of model predictions.\n",
    "        \"\"\"\n",
    "        outputs = list()\n",
    "\n",
    "        encoder_outputs, output_lengths = self.encoder(inputs, input_lengths)\n",
    "        max_length = encoder_outputs.size(1)\n",
    "\n",
    "        for encoder_output in encoder_outputs:\n",
    "            decoded_seq = self.decode(encoder_output, max_length)\n",
    "            outputs.append(decoded_seq)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1).transpose(0, 1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
