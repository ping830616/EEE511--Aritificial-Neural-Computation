{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Setup LibriSpeech Transcripts [-h] [--dir DIR] [--output OUTPUT]\n",
      "Setup LibriSpeech Transcripts: error: unrecognized arguments: -f C:\\Users\\niping1\\AppData\\Roaming\\jupyter\\runtime\\kernel-ef196151-8986-4374-ac92-a029f3959657.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# TensorSpeech/TensorFlowASR\n",
    "# TensorFlowASR/scripts/create_librispeech_trans.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "\n",
    "from tensorflow_asr.utils.utils import preprocess_paths\n",
    "\n",
    "parser = argparse.ArgumentParser(prog=\"Setup LibriSpeech Transcripts\")\n",
    "\n",
    "parser.add_argument(\"--dir\", \"-d\", type=str, default=None, help=\"Directory of dataset\")\n",
    "\n",
    "parser.add_argument(\"--output\", type=str, default=None, help=\"The output .tsv transcript file path\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "assert args.dir and args.output\n",
    "\n",
    "args.dir = preprocess_paths(args.dir)\n",
    "args.output = preprocess_paths(args.output)\n",
    "\n",
    "transcripts = []\n",
    "\n",
    "text_files = glob.glob(os.path.join(args.dir, \"**\", \"*.txt\"), recursive=True)\n",
    "\n",
    "for text_file in tqdm(text_files, desc=\"[Loading]\"):\n",
    "    current_dir = os.path.dirname(text_file)\n",
    "    with open(text_file, \"r\", encoding=\"utf-8\") as txt:\n",
    "        lines = txt.read().splitlines()\n",
    "    for line in lines:\n",
    "        line = line.split(\" \", maxsplit=1)\n",
    "        audio_file = os.path.join(current_dir, line[0] + \".flac\")\n",
    "        y, sr = librosa.load(audio_file, sr=None)\n",
    "        duration = librosa.get_duration(y, sr)\n",
    "        text = unicodedata.normalize(\"NFC\", line[1].lower())\n",
    "        transcripts.append(f\"{audio_file}\\t{duration}\\t{text}\\n\")\n",
    "\n",
    "with open(args.output, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(\"PATH\\tDURATION\\tTRANSCRIPT\\n\")\n",
    "    for line in tqdm(transcripts, desc=\"[Writing]\"):\n",
    "        out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Conformer Training [-h] [--config CONFIG] [--max_ckpts MAX_CKPTS]\n",
      "                          [--tfrecords] [--tbs TBS] [--ebs EBS]\n",
      "                          [--devices [DEVICES [DEVICES ...]]] [--mxp]\n",
      "Conformer Training: error: unrecognized arguments: -f C:\\Users\\niping1\\AppData\\Roaming\\jupyter\\runtime\\kernel-570d6ece-794c-4211-9a2c-430851990834.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# TensorFlowASR/examples/conformer/train_conformer.py /\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tensorflow_asr.utils import setup_environment, setup_strategy\n",
    "\n",
    "setup_environment()\n",
    "import tensorflow as tf\n",
    "\n",
    "DEFAULT_YAML = pd.read_csv(r\"C:\\Users\\niping1\\OneDrive\\config_modified.yml\")\n",
    "# DEFAULT_YAML = os.path.basename(\"C:\\Users\\niping1\\OneDrive\\config.yml\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "parser = argparse.ArgumentParser(prog=\"Conformer Training\")\n",
    "\n",
    "parser.add_argument(\"--config\", type=str, default=DEFAULT_YAML, help=\"The file path of model configuration file\")\n",
    "\n",
    "parser.add_argument(\"--max_ckpts\", type=int, default=10, help=\"Max number of checkpoints to keep\")\n",
    "\n",
    "parser.add_argument(\"--tfrecords\", default=False, action=\"store_true\", help=\"Whether to use tfrecords\")\n",
    "\n",
    "parser.add_argument(\"--tbs\", type=int, default=None, help=\"Train batch size per replica\")\n",
    "\n",
    "parser.add_argument(\"--ebs\", type=int, default=None, help=\"Evaluation batch size per replica\")\n",
    "\n",
    "parser.add_argument(\"--devices\", type=int, nargs=\"*\", default=[0], help=\"Devices' ids to apply distributed training\")\n",
    "\n",
    "parser.add_argument(\"--mxp\", default=False, action=\"store_true\", help=\"Enable mixed precision\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": args.mxp})\n",
    "\n",
    "strategy = setup_strategy(args.devices)\n",
    "\n",
    "from tensorflow_asr.configs.config import Config\n",
    "from tensorflow_asr.datasets.asr_dataset import ASRTFRecordDataset, ASRSliceDataset\n",
    "from tensorflow_asr.featurizers.speech_featurizers import TFSpeechFeaturizer\n",
    "from tensorflow_asr.featurizers.text_featurizers import CharFeaturizer\n",
    "from tensorflow_asr.runners.transducer_runners import TransducerTrainer\n",
    "from tensorflow_asr.models.conformer import Conformer\n",
    "from tensorflow_asr.optimizers.schedules import TransformerSchedule\n",
    "\n",
    "config = Config(args.config)\n",
    "speech_featurizer = TFSpeechFeaturizer(config.speech_config)\n",
    "text_featurizer = CharFeaturizer(config.decoder_config)\n",
    "\n",
    "if args.tfrecords:\n",
    "    train_dataset = ASRTFRecordDataset(\n",
    "        speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
    "        **vars(config.learning_config.train_dataset_config))\n",
    "    eval_dataset = ASRTFRecordDataset(\n",
    "        speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
    "        **vars(config.learning_config.eval_dataset_config))\n",
    "else:\n",
    "    train_dataset = ASRSliceDataset(\n",
    "        speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
    "        **vars(config.learning_config.train_dataset_config))\n",
    "    eval_dataset = ASRSliceDataset(\n",
    "        speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
    "        **vars(config.learning_config.eval_dataset_config))\n",
    "\n",
    "conformer_trainer = TransducerTrainer(config=config.learning_config.running_config, text_featurizer=text_featurizer, strategy=strategy)\n",
    "\n",
    "with conformer_trainer.strategy.scope():\n",
    "    # build model\n",
    "    conformer = Conformer(**config.model_config, vocabulary_size=text_featurizer.num_classes)\n",
    "    conformer._build(speech_featurizer.shape)\n",
    "    conformer.summary(line_length=120)\n",
    "\n",
    "    optimizer_config = config.learning_config.optimizer_config\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        TransformerSchedule(\n",
    "            d_model=conformer.dmodel,\n",
    "            warmup_steps=optimizer_config[\"warmup_steps\"],\n",
    "            max_lr=(0.05 / math.sqrt(conformer.dmodel))\n",
    "        ),\n",
    "        beta_1=optimizer_config[\"beta1\"],\n",
    "        beta_2=optimizer_config[\"beta2\"],\n",
    "        epsilon=optimizer_config[\"epsilon\"])\n",
    "\n",
    "conformer_trainer.compile(model=conformer, optimizer=optimizer,\n",
    "                          max_to_keep=args.max_ckpts)\n",
    "\n",
    "conformer_trainer.fit(train_dataset, eval_dataset, train_bs=args.tbs, eval_bs=args.ebs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Conformer Testing [-h] [--config CONFIG] [--saved SAVED] [--tfrecords]\n",
      "                         [--mxp] [--sentence_piece] [--device DEVICE] [--cpu]\n",
      "                         [--subwords SUBWORDS] [--output_name OUTPUT_NAME]\n",
      "Conformer Testing: error: unrecognized arguments: -f C:\\Users\\niping1\\AppData\\Roaming\\jupyter\\runtime\\kernel-570d6ece-794c-4211-9a2c-430851990834.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# TensorFlowASR/examples/conformer/test_subword_conformer.py /\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tensorflow_asr.utils import setup_environment, setup_devices\n",
    "\n",
    "setup_environment()\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# DEFAULT_YAML = pd.read_csv(r\"C:\\Users\\niping1\\OneDrive\\config_modified.yml\")\n",
    "# DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config_modified.yml\")\n",
    "current_folder = globals()['_dh'][0]\n",
    "\n",
    "# Calculating path to the input data\n",
    "data_location = os.path.join(current_folder,'config_modified.yml')\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "parser = argparse.ArgumentParser(prog=\"Conformer Testing\")\n",
    "\n",
    "parser.add_argument(\"--config\", type=str, default=DEFAULT_YAML, help=\"The file path of model configuration file\")\n",
    "\n",
    "parser.add_argument(\"--saved\", type=str, default=None, help=\"Path to saved model\")\n",
    "\n",
    "parser.add_argument(\"--tfrecords\", default=False, action=\"store_true\", help=\"Whether to use tfrecords as dataset\")\n",
    "\n",
    "parser.add_argument(\"--mxp\", default=False, action=\"store_true\", help=\"Enable mixed precision\")\n",
    "\n",
    "parser.add_argument(\"--sentence_piece\", default=False, action=\"store_true\", help=\"Whether to use `SentencePiece` model\")\n",
    "\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"Device's id to run test on\")\n",
    "\n",
    "parser.add_argument(\"--cpu\", default=False, action=\"store_true\", help=\"Whether to only use cpu\")\n",
    "\n",
    "parser.add_argument(\"--subwords\", type=str, default=None, help=\"Path to file that stores generated subwords\")\n",
    "\n",
    "parser.add_argument(\"--output_name\", type=str, default=\"test\", help=\"Result filename name prefix\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": args.mxp})\n",
    "\n",
    "setup_devices([args.device], cpu=args.cpu)\n",
    "\n",
    "from tensorflow_asr.configs.config import Config\n",
    "from tensorflow_asr.datasets.asr_dataset import ASRTFRecordDataset, ASRSliceDataset\n",
    "from tensorflow_asr.featurizers.speech_featurizers import TFSpeechFeaturizer\n",
    "from tensorflow_asr.featurizers.text_featurizers import SubwordFeaturizer, SentencePieceFeaturizer\n",
    "from tensorflow_asr.runners.base_runners import BaseTester\n",
    "from tensorflow_asr.models.conformer import Conformer\n",
    "\n",
    "config = Config(args.config)\n",
    "speech_featurizer = TFSpeechFeaturizer(config.speech_config)\n",
    "\n",
    "if args.sentence_piece:\n",
    "    print(\"Loading SentencePiece model ...\")\n",
    "    text_featurizer = SentencePieceFeaturizer.load_from_file(config.decoder_config, args.subwords)\n",
    "elif args.subwords and os.path.exists(args.subwords):\n",
    "    print(\"Loading subwords ...\")\n",
    "    text_featurizer = SubwordFeaturizer.load_from_file(config.decoder_config, args.subwords)\n",
    "else:\n",
    "    raise ValueError(\"subwords must be set\")\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "assert args.saved\n",
    "\n",
    "if args.tfrecords:\n",
    "    test_dataset = ASRTFRecordDataset(\n",
    "        speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
    "        **vars(config.learning_config.test_dataset_config)\n",
    "    )\n",
    "else:\n",
    "    test_dataset = ASRSliceDataset(\n",
    "        speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
    "        **vars(config.learning_config.test_dataset_config)\n",
    "    )\n",
    "\n",
    "# build model\n",
    "conformer = Conformer(**config.model_config, vocabulary_size=text_featurizer.num_classes)\n",
    "conformer._build(speech_featurizer.shape)\n",
    "conformer.load_weights(args.saved)\n",
    "conformer.summary(line_length=120)\n",
    "conformer.add_featurizers(speech_featurizer, text_featurizer)\n",
    "\n",
    "conformer_tester = BaseTester(\n",
    "    config=config.learning_config.running_config,\n",
    "    output_name=args.output_name\n",
    ")\n",
    "conformer_tester.compile(conformer)\n",
    "conformer_tester.run(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Vocab Training with Subwords [-h] [--config CONFIG] [--use_tf]\n",
      "                                    [--output_file OUTPUT_FILE]\n",
      "                                    [corpus [corpus ...]]\n",
      "Vocab Training with Subwords: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niping1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlowASR/scripts/generate_vocab_subwords.py \n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow_asr.configs.config import Config\n",
    "from tensorflow_asr.featurizers.text_featurizers import SubwordFeaturizer, TFSubwordFeaturizer\n",
    "\n",
    "DEFAULT_YAML = pd.read_csv(r\"C:\\Users\\niping1\\OneDrive\\config.yml\")\n",
    "#DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\n",
    "\n",
    "parser = argparse.ArgumentParser(prog=\"Vocab Training with Subwords\")\n",
    "\n",
    "parser.add_argument(\"corpus\", nargs=\"*\", type=str, default=[], help=\"Transcript files for generating subwords\")\n",
    "\n",
    "parser.add_argument(\"--config\", type=str, default=DEFAULT_YAML, help=\"The file path of model configuration file\")\n",
    "\n",
    "parser.add_argument(\"--use_tf\", default=False, action=\"store_true\", help=\"Whether to use tf subwords\")\n",
    "\n",
    "parser.add_argument(\"--output_file\", type=str, default=None, help=\"Path to file that stores generated subwords\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "config = Config(args.config)\n",
    "\n",
    "print(\"Generating subwords ...\")\n",
    "\n",
    "if not args.use_tf:\n",
    "    text_featurizer = SubwordFeaturizer.build_from_corpus(config.decoder_config, args.corpus)\n",
    "    text_featurizer.save_to_file(args.output_file)\n",
    "else:\n",
    "    TFSubwordFeaturizer.build_from_corpus(config.decoder_config, args.corpus, output_file=args.output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
